---
layout: post
title: "Less is more: language features"
date: 2015-04-13 8:16 UTC
tags: [Languages]
---
{% include JB/setup %}

<div id="post">
  <p>
    <em>Many languages have redundant features; progress in language design includes removing those features.</em>
  </p>
  <p>
    There are many programming languages, and new ones are being introduced all the time. Are these languages better than previous languages? Obviously, that's impossible to answer, since there's no clear measurement of what constitutes a 'better' programming language.
  </p>
  <p>
    Still, if you look at a historical trend, it looks as though one way to make a better language is to identify a redundant language feature, and design a new language that doesn't have that feature.
  </p>
  <p>
    <blockquote>
      "perfection is attained not when there is nothing more to add, but when there is nothing more to remove."
      - Antoine de Saint Exup√©ry
    </blockquote>
  </p>
  <p>
    In this article, you'll see various examples of language features that have already proven to be redundant, and other features where we are seeing strong indications that they are redundant.
  </p>
  <p>
    <strong>Limitless ways to shoot yourself in the foot.</strong>
  </p>
  <p>
    When the first computers were created, programs had to be written in machine code or assembly language. In machine code, you can express everything the CPU can do, because the machine code is written it terms of a CPU's instruction set. While it's possible to write correct programs in machine code, you can also write a lot of incorrect programs, including programs that crash horribly, or perhaps even destroy the machine on which they are running.
  </p>
  <p>
    You're most likely used to writing code in a higher-level language, but even so, if you share my experience with this, you'll agree that it takes a lot of trial and error to get things right. For every correct program, there are many incorrect variations. The set of incorrect programs is much bigger than the set of correct programs.
  </p>
  <p>
    With machine code, you have limitless ways to create incorrect programs. Yes: you can express everything the CPU can execute, but most of it will be incorrect. Thus, the set of valid programs is a small subset of the set of all possible instructions.
  </p>
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-01.png" alt="The set of all valid programs, inside the much larger set of all possible instructions.">
  </p>
  <p>
    Early computer programmers quickly discovered that writing in machine code was extremely error-prone, and the code was also as unreadable as it could be. One way to address this problem was to introduce assembly language, but it didn't solve the underlying problems: most assembly languages were just 'human-readable' one-to-one mappings over machine code, so you still have unlimited ways to express incorrect programs.
  </p>
  <p>
    <strong>High-level languages</strong>
  </p>
  <p>
    After having suffered with machine code and assembly language for some time, programmers figured out how to express programs in high-level languages. The first programming languages aren't in much use today, but a good example of a 'low-level' high-level language still in use today is C.
  </p>  
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-02.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of all possible instructions in a high-level language.">
  </p>
  <p>
    In C, you can express almost all valid programs. There are still tons of ways to write incorrect programs that will crash the program (or the computer), but since the language is an abstraction over machine code, there are instructions you can't express. Most of these are invalid instruction sequences anyway, so it's for the better.
  </p>
  <p>
    Notice what happened: moving from machine code to a high-level programming language removes a feature of the language. In machine code, you can express <em>anything</em>; in a high-level language, there are things you can't express. You're okay with that, because the options that were taken away from you were bad for you.
  </p>
  <p>
    <strong>GOTO</strong>
  </p>
  <p>
    In 1968, Edsger Dijkstra published his (in)famous paper <em>Go To Statement Considered Harmful</em>. In it, he argued that the GOTO statement was an bad idea, and that programs would be 'better' without the GOTO statement. This sparked a decade-long controversy, but these days we've come to understand that Dijkstra was right. Some of the most popular languages in use today (e.g. Java and JavaScript) don't have GOTO at all.
  </p>
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-03.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of all possible instructions in a high-level language without GOTO.">
  </p>
  <p>
    Since GOTO has turned out to be an entirely redundant language feature, a language without it doesn't limit your ability to express <em>valid</em> programs, but it does limit your ability to express <em>invalid</em> programs.
  </p>
  <p>
    Do you notice a pattern?
  </p>
  <p>
    Take something away, and make an improvement.
  </p>
  <p>
    There's nothing new about this; <a href="https://skillsmatter.com/skillscasts/2323-bobs-last-language">Robert C. Martin told us that years ago</a>.
  </p>
  <p>
    You can't just arbitrarily take away <em>anything</em>, because that may constrain you in such a way that there are valid programs you can no longer write:
  </p>
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-04.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of a language that takes away the wrong features.">
  </p>
  <p>
    You have to take away the <em>right</em> features.
  </p>  
  <p>
    <strong>Exceptions</strong>
  </p>
  <p>
    Today, everyone seems to agree that errors should be handled via some sort of exception mechanisms; at least, everyone can agree that <em>error codes</em> are <em>not</em> the way to go (and I agree). We need something richer than error codes, and something that balances usefulness with robustness.
  </p>
  <p>
    The problem with exceptions is that this mechanism is really only <a href="http://c2.com/cgi/wiki?DontUseExceptionsForFlowControl">GOTO statements in disguise</a> - and we've already learned that GOTO is considered harmful.
  </p>
  <p>
    A better approach is to use a <a href="http://en.wikipedia.org/wiki/Tagged_union">sum type</a> to indicate <em>either</em> <a href="http://fsharpforfunandprofit.com/posts/recipe-part2">success or failure in a composable form</a>.
  </p>
  <p>
    <strong>Pointers</strong>
  </p>
  <p>
    As Robert C. Martin also pointed out, in older languages, including C and C++, you can manipulate pointers, but as soon as you introduce polymorphism, you no longer need 'raw' pointers. Java doesn't have pointers; JavaScript doesn't do pointers; C# <em>does</em> allow you to use pointers, but I've personally never needed them outside of interop with the Windows API.
  </p>
  <p>
    As these languages have demonstrated, you don't need access to pointers in order to be able to pass values by reference. Pointers can be abstracted away.
  </p>
  <p>
    <strong>Numbers</strong>
  </p>
  <p>
    Most strongly typed languages give you an opportunity to choose between various different number types: bytes, 16-bit integers, 32-bit integers, 32-bit <em>unsigned</em> integers, single precision floating point numbers, etc. That made sense in the 1950s, but is rarely important these days; we waste time worrying about the micro-optimization it is to pick the right number type, while we lose sight of the bigger picture. As <a href="http://hanselminutes.com/396/bugs-considered-harmful-with-douglas-crockford">Douglas Crockford explains, in JavaScript there's only a <em>single</em> number type, which is a great idea - just too bad it's the wrong single number type</a>.
  </p>  
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-05.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of a language with a single, sane number type.">
  </p>
  <p>
    With the modern computer resources we have today, a programming language that would restrict you to a single, sane number type would be superior to the mess we have today.
  </p>
  <p>
    <strong>Null pointers</strong>
  </p>
  <p>
    Nulls are one of the most misunderstood language constructs. There's nothing wrong with the concept of a value that may or may not be present. Many great programming languages have this concept. In Haskell, it's called <em>Maybe</em>; in F#, it's called <em>option</em>; in T-SQL it's called <em>null</em>. What's common in all these languages is that it's an <em>opt-in</em> language feature: you can declare a value as being 'nullable', but by default, <em>it isn't</em> (nullable).
  </p>
  <p>
    However, due to <a href="http://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare">Tony Hoare's self-admitted billion-dollar mistake</a>, mainstream languages have null pointers: C, C++, Java, C#. The problem isn't the <em>concept</em> of 'null', but rather that everything <em>can</em> be null, which makes it impossible to distinguish between the cases where null is an appropriate and expected value, from the cases where null is a defect.
  </p>
  <p>
    Design a language without null pointers, and you take away the ability to produce null pointer exceptions.
  </p>  
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-06.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of a language without null pointers.">
  </p>
  <p>
    If Tony Hoare is correct that null pointers cost billions of dollars in the last few decades, getting rid of that source of defects can save you lots of money. From Turing-complete languages like T-SQL, Haskell, and F# we know that you can express all valid programs without null pointers, while a huge source of defects is removed.
  </p>
  <p>
    <strong>Mutation</strong>
  </p>
  <p>
    A central concept in Procedural, Imperative, and Object-Oriented languages is that you can change the value of a variable while executing your program. That's the reason it's called a <em>variable</em>. This seems intuitive, since a CPU contains registers, and what you actually do when you execute a program is that you move values in and out of those registers. It's also intuitive because the purpose of most programs is to change the state of the world: Store a record in a database. Send an email. Repaint the screen. Print a document. Etc.
  </p>
  <p>
    However, it turns out that mutation is also a large source of defects in software, because it makes it much harder to reason about code. Consider a line of C# code like this:
  </p>
  <p>
    <pre style="font-family:Consolas;font-size:13;color:black;"><span style="color:blue;">var</span>&nbsp;r&nbsp;=&nbsp;<span style="color:blue;">this</span>.mapper.Map(rendition);</pre>
  </p>
  <p>
    When the <code>Map</code> method returns, has <code>rendition</code> been modified? Well, if you follow <a href="http://en.wikipedia.org/wiki/Command%E2%80%93query_separation">Command Query Separation</a>, it shouldn't, but the only way you can be sure is to review the implementation of the Map method. What if that method exhibits the same problem, by calling into other methods that <em>could</em> mutate the state of the application? There's nothing in C# (or Java, or JavaScript, etc.) that prevents this from happening.
  </p>
  <p>
    In a complicated program with a big call stack, it's impossible to reason about the code, because <em>everything</em> could mutate, and once you have tens or hundreds of variables in play, you can no longer keep track of them. Did the <code>isDirty</code> flag change? Where? What about the <code>customerStatus</code>?
  </p>
  <p>
    Imagine taking away the ability to mutate state:
  </p>
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-07.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of the set of possible instructions in a language without mutation.">
  </p>
  <p>
    Most languages don't completely take away this ability, but as Haskell (a Turing complete language) demonstrates, it's possible to write any program without implicit state mutation.
  </p>
  <p>
    At this point, many people might object that Haskell is too difficult and unintuitive, but to me, that kind of argumentation is reminiscent of the resistance to removing GOTO. If you are used to relying on GOTO, you have to learn alternative ways to model the same behaviour without GOTO. Likewise, if you are used to relying on state mutation, you have to learn alternative ways to model the same behaviour without state mutation.
  </p>
  <p>
    <strong>Reference Equality</strong>
  </p>
  <p>
    In Object-Oriented languages like C# and Java, the default equality comparison is Reference Equality. If two variables point to the same memory address, the variables are considered equal. If two variables have all identical constituent values, but point to two different memory addresses, they are considered different. That's not intuitive, and many software defects are caused by this.
  </p>
  <p>
    What if you take away Reference Equality from a language?
  </p>  
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-08.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of the set of possible instructions in a language without Reference Equality.">
  </p>
  <p>
    What if all data structures instead had <a href="http://en.wikipedia.org/wiki/Relational_operator">Structural Equality</a>?
  </p>
  <p>
    This one I'm not entirely sure about, but in my experience, I almost never need Reference Equality. For Basic Correctness you never need it, but I wonder if you may need the occasional reference comparison in order to implement some performance optimizations... Still, it wouldn't hurt to switch the defaults so that Structural Equality was the default, and there was a special function you could invoke to compare two variables for Reference Equality.
  </p>
  <p>
    <strong>Inheritance</strong>
  </p>
  <p>
    Even in 2015, inheritance is everywhere, although it's more than 20 years ago the Gang of Four taught us to <a href="http://amzn.to/XBYukB">favor object composition over class inheritance</a>. It turns out that there's nothing you can do with inheritance that you can't also do with composition with interfaces. The converse doesn't hold in languages with single inheritance: there are things you can do with interfaces (such as implement more than one), that you can't do with inheritance. Composition is a superset of inheritance.
  </p>
  <p>
    This isn't just theory: in my experience, I've been able to avoid inheritance in the way I've designed my code for many years. Once you get the hang of it, it's not even difficult.
  </p>
  <p>
    <strong>Interfaces</strong>
  </p>
  <p>
    Many strongly typed languages (for example Java and C#) have interfaces, which is a mechanism to implement polymorphism. In this way, you can bundle various operations together as methods on an interface. However, one of <a href="http://www.shareasale.com/r.cfm?u=1017843&b=611266&m=53701&afftrack=&urllink=www%2Epluralsight%2Ecom%2Fcourses%2Fencapsulation%2Dsolid">the consequences of SOLID</a> is that you should favour <a href="http://martinfowler.com/bliki/RoleInterface.html">Role Interfaces</a> over <a href="http://martinfowler.com/bliki/HeaderInterface.html">Header Interfaces</a>, and as I've previously explained, <a href="http://blog.ploeh.dk/2014/03/10/solid-the-next-step-is-functional">the logical conclusion is that all interfaces should only have a single member</a>.
  </p>
  <p>
    When interfaces only have a single member, the interface declaration itself tends to mostly be in the way - the only thing that matters is the operation. <a href="http://blog.ploeh.dk/2009/05/28/DelegatesAreAnonymousInterfaces">In C#, you could just use a delegate instead</a>, and even Java has lambdas now.
  </p>
  <p>
    There's nothing new about this. Functional languages have used <em>functions</em> as the basic compositional unit for years.
  </p>
  <p>
    In my experience, you can model everything with single-member interfaces, which also implies that you can model everything with functions. Again, this isn't really surprising, since functional languages like Haskell are Turing complete too.
  </p>
  <p>
    It's possible to get by without interfaces. In fact, <a href="http://blog.cleancoder.com/uncle-bob/2015/01/08/InterfaceConsideredHarmful.html">Robert C. Martin finds them harmful</a>.
  </p>
  <p>
    <strong>Reflection</strong>
  </p>
  <p>
    If you've ever done any sort of meta-programming in .NET or Java, you probably know what Reflection is. It's a set of APIs and language or platform features that enable you to inspect, query, manipulate, or emit code.
  </p>
  <p>
    <em>Meta-programming</em> is an indispensable tool, so I'd be sorry to lose it. However, Reflection isn't the <em>only</em> way to enable meta-programming. Some languages are <a href="http://en.wikipedia.org/wiki/Homoiconicity">homoiconic</a>, which means that a program in such languages is structured as data, which itself can be queried and manipulated like any other data. Such languages don't need Reflection as a feature, because meta-programming is baked into the language, so to speak.
  </p>
  <p>
    In other words: Reflection is a language feature aimed at the goal of meta-programming. If meta-programming can be achieved via homoiconicity instead, it would imply that Reflection is a redundant feature.
  </p>
  <p>
    <strong>Cyclic Dependencies</strong>
  </p>
  <p>
    While it may be true that null pointers are the biggest single source of software defects, in my experience the greatest single source of unmaintainable code is <em>coupling</em>. One of the most problematic types of coupling is cyclic dependencies. In languages like C# and Java, cyclic dependencies are almost impossible to avoid.
  </p>
  <p>
    Here's one of my own mistakes that I only discovered because I started to look for it: in the otherwise nice and maintainable <a href="https://github.com/GreanTech/AtomEventStore">AtomEventStore</a>, there's an interface called <a href="https://github.com/GreanTech/AtomEventStore/blob/master/AtomEventStore/IXmlWritable.cs">IXmlWritable</a>:
  </p>
  <p>
    <pre style="font-family:Consolas;font-size:13;color:black;"><span style="color:blue;">public</span>&nbsp;<span style="color:blue;">interface</span>&nbsp;<span style="color:#2b91af;">IXmlWritable</span>
{
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:blue;">void</span>&nbsp;WriteTo(<span style="color:#2b91af;">XmlWriter</span>&nbsp;xmlWriter,&nbsp;<span style="color:#2b91af;">IContentSerializer</span>&nbsp;serializer);
}</pre>
  </p>
  <p>
    As you can tell, the WriteTo method takes an <a href="https://github.com/GreanTech/AtomEventStore/blob/master/AtomEventStore/IContentSerializer.cs">IContentSerializer</a> argument.
  </p>
  <p>
    <pre style="font-family:Consolas;font-size:13;color:black;"><span style="color:blue;">public</span>&nbsp;<span style="color:blue;">interface</span>&nbsp;<span style="color:#2b91af;">IContentSerializer</span>
{
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:blue;">void</span>&nbsp;Serialize(<span style="color:#2b91af;">XmlWriter</span>&nbsp;xmlWriter,&nbsp;<span style="color:blue;">object</span>&nbsp;value);
 
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color:#2b91af;">XmlAtomContent</span>&nbsp;Deserialize(<span style="color:#2b91af;">XmlReader</span>&nbsp;xmlReader);
}</pre>
  </p>
  <p>
    Notice that the Deserialize method returns an <a href="https://github.com/GreanTech/AtomEventStore/blob/master/AtomEventStore/XmlAtomContent.cs">XmlAtomContent</a> value. How is XmlAtomContent defined? Like this:
  </p>
  <p>
    <pre style="font-family:Consolas;font-size:13;color:black;"><span style="color:blue;">public</span>&nbsp;<span style="color:blue;">class</span>&nbsp;<span style="color:#2b91af;">XmlAtomContent</span>&nbsp;:&nbsp;<span style="color:#2b91af;">IXmlWritable</span></pre>
  </p>
  <p>
    Notice that it implements IXmlWritable. Oh, dear!
  </p>
  <p>
    Although I'm <em>constantly</em> on the lookout for these kinds of things, that one slipped right past me.
  </p>
  <p>
    In F# (and, I believe, OCaml), on the other hand, this wouldn't even have compiled!
  </p>
  <p>
    While F# has a way to introduce small cycles <em>within</em> a module using the <code>and</code> and <code>rec</code> keywords, there are no accidental cycles. You have to explicitly use those keywords to enable limited cycles - and there's no way to define cycles that span modules or libraries.
  </p>
  <p>
    What an excellent protection against tightly coupled code! Take away the ability to (inadvertently) introduce Cyclic Dependencies, and get a better language!
  </p>
  <p>
    <img src="/content/binary/less-is-more-language-features-figure-09.png" alt="The set of all valid programs, inside the much larger set of all possible instructions, with the overlay of the set of possible instructions in a language that disallows cycles.">
  </p>
  <p>
    This has even been shown to work in the wild: <a href="http://fsharpforfunandprofit.com/posts/cycles-and-modularity-in-the-wild">Scott Wlaschin examined C# and F# projects for cycles</a>, and found that F# projects have fewer and smaller cycles than C# projects. This analysis was later <a href="http://evelinag.com/blog/2014/06-09-comparing-dependency-networks">enhanced and corroborated by Evelina Gabasova</a>.
  </p>
  <p>
    <strong>Summary</strong>
  </p>
  <p>
    What I've tried to illustrate in this article is that there are many ways in which you could make a better language by taking away a particular feature from an existing language. Take away a redundant feature, and you'll still have a Turing complete language that can do (close to) anything, but with fewer options for shooting yourself in the foot.
  </p>
  <p>
    Perhaps the ultimate programming language is a language <em>without</em>:
    <ul>
      <li>GOTO</li>
      <li>Exceptions</li>
      <li>Pointers</li>
      <li>Lots of specialized number types</li>
      <li>Null pointers</li>
      <li>Mutation</li>
      <li>Reference equality</li>
      <li>Inheritance</li>
      <li>Interfaces</li>
      <li>Reflection</li>
      <li>Cyclic dependencies</li>
    </ul>
  </p>
  <p>
    Have I identified all possible redundant features? Most likely not, so here's a great opportunity for language designers to define an even better language, by finding something new to take away!
  </p>
</div>

<div id="comments">
<hr>
<h2 id="comments-header">Comments</h2>
  <div class="comment">
    <div class="comment-author"><a href="https://github.com/chrisdew">Chris Dew</a></div>
    <div class="comment-content">
      <p>The idea that Javascript is a superset of "valid programs", but that C is not, could do with more explanation.</p>
      <p>My background is in server code (Haskell/Java/Python/Javascript/Rust), web code (Javascript) and embedded code (Rust/C) (with some ARM assembly for when Rust/C can't produce the needed program unaided).  This makes the idea that I could express programs in Javascript, which I couldn't in C, very interesting.</p>
      <p>I think that Rust is going to be very important in embedded development, in a few years.</p>
      <p>P.S. I hope I have commented in the correct way.</p>
    </div>
    <div class="comment-date">2015-04-14 8:07 UTC</div>
  </div>
  <div class="comment">
    <div class="comment-author"><a href="http://blog.ploeh.dk/">Mark Seemann</a></div>
    <div class="comment-content">
      <p>
        Chris, thank you for writing. Where in this article do I claim that there are programs you could express in JavaScript, but not in C?
      </p>
    </div>
    <div class="comment-date">2015-04-14 14:41 UTC</div>
  </div>
  <div class="comment">
    <div class="comment-author"><a href="http://lachlanap.me/">Lachlan Phillips</a></div>
    <div class="comment-content">
      <p>
        Thanks for the post Mark. Some great points; I like especially the idea of disallowing cyclic dependencies. That'd be awesome on a legacy Java project I'm working on now!
      </p>
      <p>
        Having working in Scala and a little Haskell, I can say I love the Maybe type. One thing I have trouble visualising still is Exceptions. I think I need to find some more examples on doing without exceptions in real applications.
      </p>
      <p>
        I guess there's somewhat of a middle ground between having the "safest" language, vs the most performant language. In a lot of cases you don't need awesome performance so the safest language is the better one... but in some you do. You occasionally need control over how your bits are packed.
      </p>
      <p>
        Let's hope for the future!
      </p>
    </div>
    <div class="comment-date">2015-04-16 6:24 UTC</div>
  </div>
  <div class="comment">
    <div class="comment-author"><a href="http://blog.ploeh.dk/">Mark Seemann</a></div>
    <div class="comment-content">
      <p>
        Lachlan, thank you for writing. When it comes to working without exceptions, the point is to replace them with something stronger. Scott Wlashin's post and presentation about <a href="http://fsharpforfunandprofit.com/posts/recipe-part2">Railway Oriented Programming</a> is a great place to start. You can see another example in my <a href="http://www.infoq.com/presentations/mock-fsharp-tdd">No Mocks presentation</a>, and in one of my up-coming Pluralsight courses.
      </p>
      <p>
        When it comes to the balance between safe and performant, there will always be room for languages that sacrifice safety for speed, but in my experience, this shouldn't be a common concern. As soon as you're doing any sort of significant I/O, the cost of that tends to be so much higher than any potential imperfections in pure CPU processing.
      </p>
      <p>
        Apart from that, I don't see why, in principle, a safe language couldn't also be performant. These two traits don't seem to me to be intrinsically mutually exclusive.
      </p>
    </div>
    <div class="comment-date">2015-04-17 8:55 UTC</div>
  </div>
  <div class="comment">
    <div class="comment-author"><a href="http://practical-scheme.net/">Shiro Kawai</a></div>
    <div class="comment-content">
      <p>
        As one of less-is-more advocates, I feel your article makes sense instinctively.  However, one thing keeps bugging me: <emph>How do you define "valid programs"?</emph>
      </p>
      <p>
        I think I know what you try to mean with it, but once we try to be more specific, validity can only be defined in terms of a certain framework of semantics.  If we have such a framework, then we can derive a programming language from it, which would be a language that only accepts valid programs and reject others.  Problem solved. 
      </p>
      <p>
        In reality we don't have such a framework.  We don't know what is a valid program precisely.  There are programs that doesn't seem to make sense, and there are programs that are obviously useful, but there are huge gray area inbetween.  It's actually a moving target---the context, the environment, the user, and other outside factors will greatly affect what's valid or not.  But the fact that we don't know what valid programs shakes the ground of this whole discussion, doesn't it?  Or it can end up tautology---"valid programs are defined in terms of this language, so this language fit the best to cover the valid programs."
      </p>
    </div>
    <div class="comment-date">2015-05-14 03:09 UTC</div>
  </div>
</div>
